{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","from evaluate_quality import char_accuracy"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>base</th>\n","      <th>proc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [path, base, proc]\n","Index: []"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["documents = {\n","    'output/correio da lavoura_1484_agosto de 1945/page0001-1': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/1-facil/correio da lavoura_1484_agosto de 1945-1.txt',\n","    'output/correio da lavoura_1484_agosto de 1945/page0001-2': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/1-facil/correio da lavoura_1484_agosto de 1945-2.txt',\n","    'output/correio da lavoura_1484_agosto de 1945/page0001-3': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/1-facil/correio da lavoura_1484_agosto de 1945-3.txt',\n","    'output/correio da lavoura_10_maio_1917/page0001-1': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/2-media/correio da lavoura_10_maio_1917-1.txt',\n","    'output/correio da lavoura_10_maio_1917/page0001-2': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/2-media/correio da lavoura_10_maio_1917-2.txt',\n","    'output/correio da lavoura_459_dezembro_1925/page0001-1': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/2-media/correio da lavoura_459_dezembro_1925-1.txt',\n","    'output/correio da lavoura_10_maio_1917/page0001-3': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/3-dificil/correio da lavoura_10_maio_1917-3.txt',\n","    'output/correio da lavoura_52_marco_1918/page0001-1': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/3-dificil/correio da lavoura_52_marco_1918-1.txt',\n","    'output/correio da lavoura_52_marco_1918/page0001-2': 'C:/Users/bruno/Documents/Projetos/TCC/ground-truth/3-dificil/correio da lavoura_52_marco_1918-2.txt'\n","}\n","\n","df = pd.DataFrame(columns=['path', 'base', 'proc'])\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Char Accuracy"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def read_and_eval(ocr_path, gt_path, is_base):\n","    file = 'base.txt' if is_base else 'proc.txt'\n","    ocr_path = os.path.join(path, file)\n","    with open(ocr_path, 'r', encoding='utf8') as f:\n","        ocr = f.read()\n","    with open(gt_path, 'r', encoding='utf8') as f:\n","        gt = f.read()\n","    \n","    return char_accuracy(gt, ocr)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["output/correio da lavoura_1484_agosto de 1945/page0001-1 base\n","output/correio da lavoura_1484_agosto de 1945/page0001-1 proc\n","{'path': '1484_agosto de 1945/page0001-1', 'base': 0.7431175766504006, 'proc': 0.7312402172912255}\n","output/correio da lavoura_1484_agosto de 1945/page0001-2 base\n","output/correio da lavoura_1484_agosto de 1945/page0001-2 proc\n","{'path': '1484_agosto de 1945/page0001-2', 'base': 0.5419865642994242, 'proc': 0.6261996161228407}\n","output/correio da lavoura_1484_agosto de 1945/page0001-3 base\n","output/correio da lavoura_1484_agosto de 1945/page0001-3 proc\n","{'path': '1484_agosto de 1945/page0001-3', 'base': 0.26311801477552565, 'proc': 0.714150407274105}\n","output/correio da lavoura_10_maio_1917/page0001-1 base\n","output/correio da lavoura_10_maio_1917/page0001-1 proc\n","{'path': '10_maio_1917/page0001-1', 'base': 0, 'proc': 0.6405349036927984}\n","output/correio da lavoura_10_maio_1917/page0001-2 base\n","output/correio da lavoura_10_maio_1917/page0001-2 proc\n","{'path': '10_maio_1917/page0001-2', 'base': 0, 'proc': 0.3065596979707409}\n","output/correio da lavoura_459_dezembro_1925/page0001-1 base\n","output/correio da lavoura_459_dezembro_1925/page0001-1 proc\n","{'path': '459_dezembro_1925/page0001-1', 'base': 0.6071859175307451, 'proc': 0.6220159151193634}\n","output/correio da lavoura_10_maio_1917/page0001-3 base\n","output/correio da lavoura_10_maio_1917/page0001-3 proc\n","{'path': '10_maio_1917/page0001-3', 'base': 0.5976054138469548, 'proc': 0.27511712649661635}\n","output/correio da lavoura_52_marco_1918/page0001-1 base\n","output/correio da lavoura_52_marco_1918/page0001-1 proc\n","{'path': '52_marco_1918/page0001-1', 'base': 0, 'proc': 0.5838103025347506}\n","output/correio da lavoura_52_marco_1918/page0001-2 base\n","output/correio da lavoura_52_marco_1918/page0001-2 proc\n","{'path': '52_marco_1918/page0001-2', 'base': 0.7537160658655986, 'proc': 0.509479305740988}\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>base</th>\n","      <th>proc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1484_agosto de 1945/page0001-1</td>\n","      <td>0.743118</td>\n","      <td>0.731240</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1484_agosto de 1945/page0001-2</td>\n","      <td>0.541987</td>\n","      <td>0.626200</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1484_agosto de 1945/page0001-3</td>\n","      <td>0.263118</td>\n","      <td>0.714150</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10_maio_1917/page0001-1</td>\n","      <td>0.000000</td>\n","      <td>0.640535</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10_maio_1917/page0001-2</td>\n","      <td>0.000000</td>\n","      <td>0.306560</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>459_dezembro_1925/page0001-1</td>\n","      <td>0.607186</td>\n","      <td>0.622016</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>10_maio_1917/page0001-3</td>\n","      <td>0.597605</td>\n","      <td>0.275117</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>52_marco_1918/page0001-1</td>\n","      <td>0.000000</td>\n","      <td>0.583810</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>52_marco_1918/page0001-2</td>\n","      <td>0.753716</td>\n","      <td>0.509479</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             path      base      proc\n","0  1484_agosto de 1945/page0001-1  0.743118  0.731240\n","1  1484_agosto de 1945/page0001-2  0.541987  0.626200\n","2  1484_agosto de 1945/page0001-3  0.263118  0.714150\n","3         10_maio_1917/page0001-1  0.000000  0.640535\n","4         10_maio_1917/page0001-2  0.000000  0.306560\n","5    459_dezembro_1925/page0001-1  0.607186  0.622016\n","6         10_maio_1917/page0001-3  0.597605  0.275117\n","7        52_marco_1918/page0001-1  0.000000  0.583810\n","8        52_marco_1918/page0001-2  0.753716  0.509479"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["for path in documents:\n","    print(path, 'base')\n","    base = read_and_eval(path, documents[path], True)\n","    print(path, 'proc')\n","    proc = read_and_eval(path, documents[path], False)\n","    print({ 'path': path[26:], 'base': base, 'proc': proc })\n","    df = df.append({ 'path': path[26:], 'base': base, 'proc': proc }, ignore_index=True)\n","\n","df"]},{"cell_type":"markdown","metadata":{},"source":["## Semantic Similarity"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import numpy as np\n","from gensim.models import Word2Vec\n","from gensim.models import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.corpus import mac_morpho\n","from nltk.stem.rslp import RSLPStemmer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from unidecode import unidecode"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_data(path):\n","    documents_list = []\n","    with open(path ,\"r\", encoding='utf8') as f:\n","        for line in f.readlines():\n","            text = line.strip()\n","            if len(text) > 0:\n","                documents_list.append(text)\n","    \n","    return documents_list"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def preprocess_data(doc_set):\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    stop = set(stopwords.words('portuguese'))\n","    stemmer = RSLPStemmer()\n","    \n","    texts = []\n","    for i in doc_set:\n","        raw = unidecode(i).lower()\n","        tokens = tokenizer.tokenize(raw)\n","        stopped_tokens = [i for i in tokens if not i in stop]\n","        stemmed_tokens = [stemmer.stem(i) for i in stopped_tokens]\n","        texts.append(stemmed_tokens)\n","    \n","    return texts"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["corpus = []\n","for ocr_path in documents:\n","    gt_path = documents[ocr_path]\n","    doc = load_data(gt_path)\n","    corpus.extend(preprocess_data(doc))\n","\n","model = Word2Vec(corpus)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def load_preprocess_and_flatten(path):\n","    data = load_data(path)\n","    clean = preprocess_data(data)\n","    return [item for sublist in clean for item in sublist]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def get_vector(model, s):\n","    return np.mean(np.array([model[i] for i in s if i in model]), axis=0)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>base</th>\n","      <th>proc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1484_agosto de 1945/page0001-1</td>\n","      <td>0.999543</td>\n","      <td>0.999644</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1484_agosto de 1945/page0001-2</td>\n","      <td>0.998447</td>\n","      <td>0.998700</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1484_agosto de 1945/page0001-3</td>\n","      <td>0.998253</td>\n","      <td>0.998988</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10_maio_1917/page0001-1</td>\n","      <td>NaN</td>\n","      <td>0.998723</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10_maio_1917/page0001-2</td>\n","      <td>NaN</td>\n","      <td>0.998858</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>459_dezembro_1925/page0001-1</td>\n","      <td>0.999168</td>\n","      <td>0.999184</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>10_maio_1917/page0001-3</td>\n","      <td>0.997555</td>\n","      <td>0.987752</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>52_marco_1918/page0001-1</td>\n","      <td>NaN</td>\n","      <td>0.998301</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>52_marco_1918/page0001-2</td>\n","      <td>0.998980</td>\n","      <td>0.999403</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             path      base      proc\n","0  1484_agosto de 1945/page0001-1  0.999543  0.999644\n","1  1484_agosto de 1945/page0001-2  0.998447  0.998700\n","2  1484_agosto de 1945/page0001-3  0.998253  0.998988\n","3         10_maio_1917/page0001-1       NaN  0.998723\n","4         10_maio_1917/page0001-2       NaN  0.998858\n","5    459_dezembro_1925/page0001-1  0.999168  0.999184\n","6         10_maio_1917/page0001-3  0.997555  0.987752\n","7        52_marco_1918/page0001-1       NaN  0.998301\n","8        52_marco_1918/page0001-2  0.998980  0.999403"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_sim = pd.DataFrame(columns=['path', 'base', 'proc'])\n","for ocr_path in documents:\n","    gt_path = documents[ocr_path]\n","    gt_flat = load_preprocess_and_flatten(gt_path)\n","\n","    base_path = os.path.join(ocr_path, 'base.txt')\n","    base_flat = load_preprocess_and_flatten(base_path)\n","\n","    proc_path = os.path.join(ocr_path, 'proc.txt')\n","    proc_flat = load_preprocess_and_flatten(proc_path)\n","    \n","    gt_vector = get_vector(model.wv, gt_flat)\n","    base_vector = get_vector(model.wv, base_flat)\n","    proc_vector = get_vector(model.wv, proc_flat)\n","\n","    base_similarity = np.mean(cosine_similarity(gt_vector.reshape(1,-1), base_vector.reshape(1,-1))) if base_vector.reshape(1,-1).shape[1] == 100 else np.nan\n","    proc_similarity = np.mean(cosine_similarity(gt_vector.reshape(1,-1), proc_vector.reshape(1,-1))) if proc_vector.reshape(1,-1).shape[1] == 100 else np.nan\n","    df_sim = df_sim.append({ 'path': ocr_path[26:], 'base': base_similarity, 'proc': proc_similarity }, ignore_index=True)\n","\n","df_sim"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["corpus = []\n","for ocr_path in documents:\n","    gt_path = documents[ocr_path]\n","    doc = load_data(gt_path)\n","    clean = preprocess_data(doc)\n","    clean_flat = [item for sublist in clean for item in sublist]\n","    corpus.append(TaggedDocument(clean_flat, [ocr_path]))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["['Augusto',\n"," 'Ribeiro',\n"," 'Garcia',\n"," 'é',\n"," 'jornalista',\n"," ',',\n"," 'advogado',\n"," 'agrarista',\n"," 'e',\n"," 'membro',\n"," 'de',\n"," 'o',\n"," 'Instituto',\n"," 'Paulista',\n"," 'de',\n"," 'Direito',\n"," 'Agrário']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["paras = mac_morpho.paras()\n","mac_morpho_corpus = []\n","for i in range(len(paras)):\n","    mac_morpho_corpus.append([item for sublist in paras[i] for item in sublist])\n","# mac_morpho_corpus[41]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["for i, doc in enumerate(mac_morpho_corpus):\n","    clean = preprocess_data(doc)\n","    clean_flat = [item for sublist in clean for item in sublist]\n","    corpus.append(TaggedDocument(clean_flat, [f'mac_morpho{i}']))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["d2v = Doc2Vec(vector_size=100, min_count=2, epochs=10)\n","d2v.build_vocab(corpus)\n","d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>base</th>\n","      <th>proc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1484_agosto de 1945/page0001-1</td>\n","      <td>0.964087</td>\n","      <td>0.951811</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1484_agosto de 1945/page0001-2</td>\n","      <td>0.943627</td>\n","      <td>0.940309</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1484_agosto de 1945/page0001-3</td>\n","      <td>0.867083</td>\n","      <td>0.941844</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10_maio_1917/page0001-1</td>\n","      <td>0.113475</td>\n","      <td>0.938927</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10_maio_1917/page0001-2</td>\n","      <td>0.110400</td>\n","      <td>0.962401</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>459_dezembro_1925/page0001-1</td>\n","      <td>0.927177</td>\n","      <td>0.874901</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>10_maio_1917/page0001-3</td>\n","      <td>0.959448</td>\n","      <td>0.858907</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>52_marco_1918/page0001-1</td>\n","      <td>-0.061419</td>\n","      <td>0.889602</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>52_marco_1918/page0001-2</td>\n","      <td>0.953332</td>\n","      <td>0.954203</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             path      base      proc\n","0  1484_agosto de 1945/page0001-1  0.964087  0.951811\n","1  1484_agosto de 1945/page0001-2  0.943627  0.940309\n","2  1484_agosto de 1945/page0001-3  0.867083  0.941844\n","3         10_maio_1917/page0001-1  0.113475  0.938927\n","4         10_maio_1917/page0001-2  0.110400  0.962401\n","5    459_dezembro_1925/page0001-1  0.927177  0.874901\n","6         10_maio_1917/page0001-3  0.959448  0.858907\n","7        52_marco_1918/page0001-1 -0.061419  0.889602\n","8        52_marco_1918/page0001-2  0.953332  0.954203"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df_sim = pd.DataFrame(columns=['path', 'base', 'proc'])\n","for ocr_path in documents:\n","    gt_path = documents[ocr_path]\n","    gt_flat = load_preprocess_and_flatten(gt_path)\n","\n","    base_path = os.path.join(ocr_path, 'base.txt')\n","    base_flat = load_preprocess_and_flatten(base_path)\n","\n","    proc_path = os.path.join(ocr_path, 'proc.txt')\n","    proc_flat = load_preprocess_and_flatten(proc_path)\n","    \n","    # gt_vector = d2v.infer_vector(gt_flat)\n","    # base_vector = d2v.infer_vector(base_flat)\n","    # proc_vector = d2v.infer_vector(proc_flat)\n","\n","    base_similarity = d2v.similarity_unseen_docs(gt_flat, base_flat)\n","    proc_similarity = d2v.similarity_unseen_docs(gt_flat, proc_flat)\n","    # base_similarity = np.mean(cosine_similarity(gt_vector.reshape(1,-1), base_vector.reshape(1,-1))) if base_vector.reshape(1,-1).shape[1] == 100 else np.nan\n","    # proc_similarity = np.mean(cosine_similarity(gt_vector.reshape(1,-1), proc_vector.reshape(1,-1))) if proc_vector.reshape(1,-1).shape[1] == 100 else np.nan\n","    df_sim = df_sim.append({ 'path': ocr_path[26:], 'base': base_similarity, 'proc': proc_similarity }, ignore_index=True)\n","\n","df_sim"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"003ac9aaa1485d5b771c302278ee1fa2d675754790301e4c4d0b7cf7d8ab189c"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
